{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_news = pd.read_csv(\"processed_news_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "all_news[\"tokens\"] = all_news[\"tokens\"].apply(literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_news[\"bigrams\"] = all_news[\"bigrams\"].apply(literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_spacy_doc(row):\n",
    "    print(\"we are at \",str(row[\"date\"])\n",
    "    return nlp(row[\"sentences\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_news[\"sentence_en\"] = all_news.apply(make_spacy_doc,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(all_news[\"sentence_en\"].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bigram(row):\n",
    "    print(\"reaching \"+str(row[\"date\"]))\n",
    "    a = []\n",
    "    #print(len(row[\"tokens\"]))\n",
    "    for i in range(len(row[\"tokens\"])-1):\n",
    "        this_bigram = row[\"tokens\"][i]+\"_\"+row[\"tokens\"][i+1]\n",
    "        a.append(this_bigram)\n",
    "    \n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backup_copy = all_news.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "get_bigram(all_news.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_news[\"bigrams\"] = all_news.apply(get_bigram,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_news[\"date\"] = pd.to_datetime(all_news[\"date\"], format=\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backup_all_news = all_news.copy(deep=True)\n",
    "usable_all_news = all_news.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = len(all_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_news = all_news[0: int(length*4/5)]\n",
    "test_news = all_news[int(length*4/5):length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_news = np.array(train_news)\n",
    "sentences = processed_news[:,3]\n",
    "tokens = processed_news[:,4]\n",
    "sentence_handler = processed_news[:,5]\n",
    "bigrams = processed_news[:,6]\n",
    "company_symbol = processed_news[:,7]\n",
    "company_name = processed_news[:,8]\n",
    "company_processed_name = processed_news[:,9]\n",
    "roc_future_past1 = 100*(processed_news[:,10] - processed_news[:,11])/processed_news[:,11]\n",
    "roc_past1_past2 = 100*(processed_news[:,11] - processed_news[:,12])/processed_news[:,12]\n",
    "roc_past2_past3 = 100*(processed_news[:,12] - processed_news[:,13])/processed_news[:,13]\n",
    "price_movement=processed_news[:,14]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "model=Word2Vec(sentences=tokens,min_count=2)\n",
    "bag_of_keywords=set(['rise','drop','fall','surge','shrink','jump','slump','reduce','outperform','progress','up','down'])\n",
    "max_size=1000\n",
    "\n",
    "for i in range(10):\n",
    "    new_keywords=[]\n",
    "\n",
    "    for word in bag_of_keywords:\n",
    "        \n",
    "        if word in model.wv.vocab.keys():\n",
    "            \n",
    "            new_keywords.extend(model.most_similar(word))\n",
    "            print(len(new_keywords),model.most_similar(word))\n",
    "            \n",
    "    for new in new_keywords:\n",
    "        if (new[0].isalpha()) and (new[0].islower()) and (len(new[0])>3):\n",
    "            bag_of_keywords.add(new[0])\n",
    "            \n",
    "            if len(bag_of_keywords) == max_size:\n",
    "                \n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_keywords=np.array(list(bag_of_keywords))\n",
    "bag_of_keywords.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec,phrases\n",
    "\n",
    "from gensim.models.phrases import Phrases\n",
    "\n",
    "all_bigrams = train_news[\"tokens\"].iloc[:].values\n",
    "bi = Phrases(all_bigrams)\n",
    "model_bigram =Word2Vec(bi[all_bigrams],min_count=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_bigram_keywords=set(['worse_than','percent_drop','stronger_than','economic_downturn',\"further_damage\",\"further_damage\",\"economic_recover\",\"help_boost\",\"well_above\",\"market_dominance\"])\n",
    "\n",
    "max_size_bigram = 200\n",
    "\n",
    "for i in range(10):\n",
    "    print(i)\n",
    "    new_bigram_keywords=[]\n",
    "\n",
    "    for word in bag_of_bigram_keywords:\n",
    "        \n",
    "        if word in model_bigram.wv.vocab.keys():\n",
    "            \n",
    "            new_bigram_keywords.extend(model_bigram.most_similar(word))\n",
    "            print(len(new_bigram_keywords),model_bigram.most_similar(word))\n",
    "            \n",
    "    for new in new_bigram_keywords:\n",
    "        \n",
    "        if (new[0].isalpha()) and (new[0].islower()) and (len(new[0])>3):\n",
    "            bag_of_bigram_keywords.add(new[0])\n",
    "            \n",
    "            if len(bag_of_bigram_keywords) == max_size_bigram:\n",
    "                \n",
    "                break\n",
    "    if len(bag_of_bigram_keywords) == max_size_bigram:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bag_of_bigram_keywords =np.array(list(bag_of_bigram_keywords)) \n",
    "bag_of_bigram_keywords.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unigram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(lowercase=False,min_df=1,use_idf=True,vocabulary=bag_of_keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_tfidf = tfidf_vectorizer.fit_transform(sentences)\n",
    "news_tfidf = news_tfidf.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_frequency = np.sum(news_tfidf,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_word_frequency = np.sum(news_tfidf[price_movement[:]==1.0],axis=0)*news_tfidf.shape[0]\n",
    "positive_word_frequency = np.array(positive_word_frequency).reshape(news_tfidf.shape[1],)\n",
    "positive_word_frequency[positive_word_frequency == 0] = 1\n",
    "frequency_positive = np.sum(price_movement[:]==1.0)\n",
    "PMI_positive = np.log(positive_word_frequency*sentences.shape[0]/\\\n",
    "                      word_frequency*frequency_positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_word_frequency = np.sum(news_tfidf[price_movement[:]==-1.0],axis=0)*news_tfidf.shape[0]\n",
    "negative_word_frequency = np.array(negative_word_frequency).reshape(news_tfidf.shape[1],)\n",
    "negative_word_frequency[negative_word_frequency==0] = 1\n",
    "frequency_negative = np.sum(price_movement[:]==-1.0)\n",
    "PMI_negative = np.log(negative_word_frequency*sentences.shape[0]/\\\n",
    "                      word_frequency*frequency_negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Polarity_Score = PMI_positive - PMI_negative\n",
    "Polarity_Score.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Polarity_Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tfidf = TfidfVectorizer(lowercase=False,min_df=1,vocabulary=bag_of_keywords,use_idf=False)\n",
    "X_tfidf_with_keywords=train_tfidf.fit_transform(sentences)\n",
    "X_tfidf_with_keywords=X_tfidf_with_keywords.toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tfidf_with_keywords.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,h in enumerate(sentence_handler):\n",
    "    for nc in h.noun_chunks:\n",
    "        if company_symbol[i] in nc.text \\\n",
    "        or company_name[1] in nc.text \\\n",
    "        or company_processed_name[2] in nc.text:\n",
    "            if nc.root.head.text in bag_of_keywords:\n",
    "                \n",
    "                j = np.where(bag_of_keywords==nc.root.head.text)[0][0]\n",
    "                \n",
    "                if nc.root.dep_ == spacy.symbols.nsubj:\n",
    "                    \n",
    "                    X_tfidf_with_keywords[i,j]=X_tfidf_with_keywords[i,j]*Polarity_Score[j]\n",
    "                    \n",
    "                if nc.root.dep_ == spacy.symbols.nsubjpass:\n",
    "                    \n",
    "                    X_tfidf_with_keywords[i,j]=X_tfidf_with_keywords[i,j]*(-Polarity_Score[j])\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "numOfrow = price_movement.shape[0]\n",
    "\n",
    "Rate_Of_Change_Array =np.zeros((numOfrow,2))\n",
    "\n",
    "for i in range(len(roc_future_past1[:])):\n",
    "    \n",
    "    Rate_Of_Change_Array[i]=[roc_past1_past2[i],roc_past2_past3[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_data_final = np.concatenate((X_tfidf_with_keywords,Rate_Of_Change_Array),axis=1)\n",
    "X_train_data_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_stock_price_movement = np.array(price_movement[:],dtype='int32')\n",
    "Y_stock_price_movement[Y_stock_price_movement == -1] = 0\n",
    "Y_stock_price_movement = to_categorical(Y_stock_price_movement,num_classes=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.layers import Dense,Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of days when stock price increases: \", np.sum(Y_stock_price_movement[:,1] == 1) )\n",
    "print(\"Number of days when stock price decreases: \", np.sum(Y_stock_price_movement[:,0] == 1) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DNN_model=Sequential()\n",
    "input_dimension = X_train_data_final.shape[1]\n",
    "DNN_model.add(Dense(512, activation='relu',input_dim = input_dimension))\n",
    "DNN_model.add(Dropout(0.5))\n",
    "DNN_model.add(Dense(512,activation='relu'))\n",
    "DNN_model.add(Dropout(0.5))\n",
    "DNN_model.add(Dense(512,activation='relu'))\n",
    "\n",
    "DNN_model.add(Dense(2,activation='softmax'))\n",
    "DNN_model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "DNN_model.fit(X_train_data_final, Y_stock_price_movement,\n",
    "          batch_size=128,\n",
    "          epochs=5,\n",
    "          verbose=1,\n",
    "          validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_news_test = np.array(test_news)\n",
    "sentences_test = processed_news_test[:,3]\n",
    "tokens_test = processed_news_test[:,4]\n",
    "sentence_handler_test = processed_news_test[:,5]\n",
    "bigrams_test = processed_news_test[:,6]\n",
    "company_symbol_test = processed_news_test[:,7]\n",
    "company_name_test = processed_news_test[:,8]\n",
    "company_processed_name_test = processed_news_test[:,9]\n",
    "roc_future_past1_test = 100*(processed_news_test[:,10] - processed_news_test[:,11])/processed_news_test[:,11]\n",
    "roc_past1_past2_test = 100*(processed_news_test[:,11] - processed_news_test[:,12])/processed_news_test[:,12]\n",
    "roc_past2_past3_test = 100*(processed_news_test[:,12] - processed_news_test[:,13])/processed_news_test[:,13]\n",
    "price_movement_test = processed_news_test[:,14]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_tfidf_test = TfidfVectorizer(lowercase=False,min_df=1,vocabulary=bag_of_keywords,use_idf=False)\n",
    "X_full_tfidf_test = full_tfidf_test.fit_transform(sentences_test)\n",
    "X_full_tfidf_test = X_full_tfidf_test.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,h in enumerate(sentence_handler_test):\n",
    "    for nouns in h.noun_chunks:\n",
    "        if company_symbol_test[i] in nouns.text \\\n",
    "        or company_name_test[1] in nouns.text \\\n",
    "        or company_processed_name_test[2] in nouns.text:\n",
    "            if nouns.root.head.text in bag_of_keywords:\n",
    "                \n",
    "                j=np.where(bag_of_keywords==nouns.root.head.text)[0][0]\n",
    "                \n",
    "                if nouns.root.dep_ == spacy.symbols.nsubj:\n",
    "                    X_full_tfidf_test[i,j]=X_full_tfidf_test[i,j]*Polarity_Score[j]\n",
    "                    \n",
    "                if nouns.root.dep_ == spacy.symbols.nsubjpass:\n",
    "                    X_full_tfidf_test[i,j]=X_full_tfidf_test[i,j]*(-Polarity_Score[j])\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "numOfrow_test = price_movement_test.shape[0]\n",
    "\n",
    "Rate_Of_Change_Array_Test =np.zeros((numOfrow_test,2))\n",
    "\n",
    "for i in range(len(roc_future_past1_test[:])):\n",
    "    \n",
    "    Rate_Of_Change_Array_Test[i]=[roc_past1_past2_test[i],roc_past2_past3_test[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_data_final = np.concatenate((X_full_tfidf_test,Rate_Of_Change_Array_Test),axis=1)\n",
    "X_test_data_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_true_stock_price_movement = np.array(price_movement_test[:],dtype = 'int32')\n",
    "Y_true_stock_price_movement[Y_true_stock_price_movement == -1] = 0\n",
    "Y_true_stock_price_movement = Y_true_stock_price_movement.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = DNN_model.predict_classes(X_test_data_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "test_accuracy = accuracy_score(Y_true_stock_price_movement,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bigram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer_bigram = TfidfVectorizer(lowercase=False,min_df=1,use_idf=True,vocabulary=bag_of_bigram_keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_tfidf_bigram = tfidf_vectorizer_bigram.fit_transform(sentences)\n",
    "news_tfidf_bigram = news_tfidf_bigram.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_frequency = np.sum(news_tfidf_bigram,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_bigram_frequency = np.array(np.sum(news_tfidf_bigram[price_movement[:]==1.0],axis=0)*news_tfidf_bigram.shape[0]).reshape(news_tfidf_bigram.shape[1],)\n",
    "positive_bigram_frequency[positive_bigram_frequency == 0] = 1\n",
    "bigram_frequency_positive = np.sum(price_movement[:]==1.0)\n",
    "PMI_positive_bigram = np.log(positive_bigram_frequency*sentences.shape[0]/\\\n",
    "                      bigram_frequency*bigram_frequency_positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_bigram_frequency = np.array(np.sum(news_tfidf_bigram[price_movement[:]==-1.0],axis=0)*news_tfidf_bigram.shape[0]).reshape(news_tfidf_bigram.shape[1],)\n",
    "negative_bigram_frequency[negative_bigram_frequency==0] = 1\n",
    "bigram_frequency_negative = np.sum(price_movement[:]==-1.0)\n",
    "PMI_negative_bigram = np.log(negative_bigram_frequency*sentences.shape[0]/\\\n",
    "                             bigram_frequency*bigram_frequency_negative)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Polarity_Score_bigram = PMI_positive_bigram - PMI_negative_bigram\n",
    "Polarity_Score_bigram.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tfidf_bigram = TfidfVectorizer(lowercase=False,min_df=1,vocabulary=bag_of_bigram_keywords,use_idf=False)\n",
    "X_tfidf_with_bigram_keywords = train_tfidf_bigram.fit_transform(sentences)\n",
    "X_tfidf_with_bigram_keywords = X_tfidf_with_bigram_keywords.toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tfidf_with_bigram_keywords.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,h in enumerate(sentence_handler):\n",
    "    for nouns in h.noun_chunks:\n",
    "        if company_symbol[i] in nouns.text \\\n",
    "        or company_name[1] in nouns.text \\\n",
    "        or company_processed_name[2] in nouns.text:\n",
    "            if nouns.root.head.text in bag_of_bigram_keywords:\n",
    "                \n",
    "                j = np.where(bag_of_bigram_keywords==nouns.root.head.text)[0][0]\n",
    "                \n",
    "                if nouns.root.dep_ == spacy.symbols.nsubj:\n",
    "                    X_tfidf_with_bigram_keywords[i,j]=X_tfidf_with_bigram_keywords[i,j]*Polarity_Score_bigram[j]\n",
    "                    \n",
    "                if nouns.root.dep_ == spacy.symbols.nsubjpass:\n",
    "                    X_tfidf_with_bigram_keywords[i,j]=X_tfidf_with_bigram_keywords[i,j]*(-Polarity_Score_bigram[j])\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_bigram_train_data_final = np.concatenate((X_tfidf_with_bigram_keywords,Rate_Of_Change_Array),axis=1)\n",
    "X_bigram_train_data_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DNN_bigram_model=Sequential()\n",
    "input_dimension = X_bigram_train_data_final.shape[1]\n",
    "DNN_bigram_model.add(Dense(512, activation='relu',input_dim = input_dimension))\n",
    "DNN_bigram_model.add(Dropout(0.5))\n",
    "DNN_bigram_model.add(Dense(512,activation='relu'))\n",
    "DNN_bigram_model.add(Dropout(0.5))\n",
    "DNN_bigram_model.add(Dense(512,activation='relu'))\n",
    "\n",
    "DNN_bigram_model.add(Dense(2,activation='softmax'))\n",
    "DNN_bigram_model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "DNN_bigram_model.fit(X_bigram_train_data_final, Y_stock_price_movement,\n",
    "          batch_size=128,\n",
    "          epochs=5,\n",
    "          verbose=1,\n",
    "          validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model With Features: Unigram, Bigram and Event Tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_tags = set(['published','investment','bankrupt','government','acquisition','suit'])\n",
    "\n",
    "category_size=100\n",
    "\n",
    "for i in range(10):\n",
    "    new_words=[]\n",
    "\n",
    "    for tag in category_tags:\n",
    "        \n",
    "        if tag in model.wv.vocab.keys():\n",
    "            new_words.extend(model.most_similar(tag))\n",
    "            \n",
    "    for word in new_words:\n",
    "        if (word[0].isalpha()) \\\n",
    "        and (word[0].islower()) \\\n",
    "        and (len(word[0])>3):\n",
    "            #print(word[0])\n",
    "            category_tags.add(word[0])\n",
    "            if len(category_tags) == category_size:\n",
    "                break\n",
    "    if len(category_tags) == category_size:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_tags = np.array(list(category_tags))\n",
    "category_tags.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_vectorizer = CountVectorizer(lowercase=False,min_df=1,vocabulary=category_tags)\n",
    "X_category_vectorizer= category_vectorizer.fit_transform(sentences)\n",
    "X_category_vectorizer=X_category_vectorizer.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_tfidf=TfidfVectorizer(lowercase=False,min_df=1,vocabulary=category_tags)\n",
    "X_category_tfidf=category_tfidf.fit_transform(sentences)\n",
    "X_category_tfidf=X_category_tfidf.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_category_frequency=np.array(np.sum(X_category_tfidf[price_movement[:]==1.0],axis=0)*X_category_tfidf.shape[0]).reshape(X_category_tfidf.shape[1],)\n",
    "positive_category_frequency[positive_category_frequency==0]=1\n",
    "category_frequency=np.sum(X_category_tfidf,axis=0)\n",
    "frequency_cat_positive=np.sum(price_movement[:]==1.0)\n",
    "category_PMI_pos=np.log(positive_category_frequency*sentences.shape[0]/category_frequency*frequency_cat_positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_category_frequency=np.array(np.sum(X_category_tfidf[price_movement[:]==-1.0],axis=0)*X_category_tfidf.shape[0]).reshape(X_category_tfidf.shape[1],)\n",
    "negative_category_frequency[negative_category_frequency==0]=1\n",
    "category_frequency=np.sum(X_category_tfidf,axis=0)\n",
    "frequency_cat_negative=np.sum(price_movement[:]==-1.0)\n",
    "category_PMI_neg=np.log(negative_category_frequency*sentences.shape[0]/category_frequency*frequency_cat_negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polarity_score_category = category_PMI_pos - category_PMI_neg\n",
    "polarity_score_category.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_feature = np.concatenate((bag_of_keywords,bag_of_bigram_keywords,category_tags))\n",
    "full_feature,full_index=np.unique(full_feature,return_index=True)\n",
    "polarity_score_all =np.concatenate((Polarity_Score,Polarity_Score_bigram,polarity_score_category))[full_index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tfidf = TfidfVectorizer(lowercase=False,min_df=1,vocabulary=full_feature,use_idf=False)\n",
    "X_all_tfidf=all_tfidf.fit_transform(sentences)\n",
    "X_all_tfidf=X_all_tfidf.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,h in enumerate(sentence_handler):\n",
    "    for nc in h.noun_chunks:\n",
    "        if company_symbol[i] in nc.text or company_name[1] in nc.text or company_processed_name[2] in nc.text:\n",
    "            if nc.root.head.text in full_feature:\n",
    "                \n",
    "                j = np.where(full_feature==nc.root.head.text)[0][0]\n",
    "                \n",
    "                if nc.root.dep_ == spacy.symbols.nsubj:\n",
    "                    X_all_tfidf[i,j]=X_all_tfidf[i,j]*polarity_score_all[j]\n",
    "                    \n",
    "                if nc.root.dep_ == spacy.symbols.nsubjpass:\n",
    "                    X_all_tfidf[i,j]=X_all_tfidf[i,j]*(-polarity_score_all[j])\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_all_features =np.concatenate((X_all_tfidf,Rate_Of_Change_Array),axis=1)\n",
    "X_train_all_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DNN_model_all_features =Sequential()\n",
    "input_dimension = X_train_all_features.shape[1]\n",
    "DNN_model_all_features.add(Dense(480, activation='relu',input_dim = input_dimension))\n",
    "DNN_model_all_features.add(Dropout(0.5))\n",
    "DNN_model_all_features.add(Dense(480,activation='relu'))\n",
    "DNN_model_all_features.add(Dropout(0.5))\n",
    "DNN_model_all_features.add(Dense(480,activation='relu'))\n",
    "\n",
    "DNN_model_all_features.add(Dense(2,activation='softmax'))\n",
    "DNN_model_all_features.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "DNN_model_all_features.fit(X_train_all_features, Y_stock_price_movement,\n",
    "          batch_size=128,\n",
    "          epochs=5,\n",
    "          verbose=1,\n",
    "          validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test (Unigram+Bigram+Event Tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_tfidf_test_all = TfidfVectorizer(lowercase=False,min_df=1,vocabulary=full_feature,use_idf=False)\n",
    "X_full_tfidf_test_all = full_tfidf_test_all.fit_transform(sentences_test)\n",
    "X_full_tfidf_test_all = X_full_tfidf_test_all.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,h in enumerate(sentence_handler_test):\n",
    "    for nouns in h.noun_chunks:\n",
    "        if company_symbol_test[i] in nouns.text \\\n",
    "        or company_name_test[1] in nouns.text \\\n",
    "        or company_processed_name_test[2] in nouns.text:\n",
    "            \n",
    "            if nouns.root.head.text in full_feature:\n",
    "                \n",
    "                j=np.where(full_feature==nouns.root.head.text)[0][0]\n",
    "                \n",
    "                if nouns.root.dep_ == spacy.symbols.nsubj:\n",
    "                    X_full_tfidf_test_all[i,j]=X_full_tfidf_test_all[i,j]*polarity_score_all[j]\n",
    "                    \n",
    "                if nouns.root.dep_ == spacy.symbols.nsubjpass:\n",
    "                    X_full_tfidf_test_all[i,j]=X_full_tfidf_test_all[i,j]*(-polarity_score_all[j])\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_data_all_feature = np.concatenate((X_full_tfidf_test_all,Rate_Of_Change_Array_Test),axis=1)\n",
    "X_test_data_all_feature.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_all_feature = DNN_model_all_features.predict_classes(X_test_data_all_feature)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "test_accuracy_all_feature = accuracy_score(Y_true_stock_price_movement,predictions_all_feature.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy_all_feature"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
